{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465c003a-29cf-4cfe-a0c6-d36c04ae2b37",
   "metadata": {},
   "source": [
    "# Conversational RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a73fbf-e241-499d-a235-32a87b46ee7c",
   "metadata": {},
   "source": [
    "## Intro\n",
    "* In most RAG applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8ef906-9484-4b29-a963-e64e9ba68d3b",
   "metadata": {},
   "source": [
    "## The problem\n",
    "* How do we handle when the user refers to previous Q&As in the conversation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ddb93-56cf-4f09-941e-16ea1d0812e3",
   "metadata": {},
   "source": [
    "## The second problem...\n",
    "* This is probably the topic that is worst explained in the LangChain documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3e3967-d201-458f-adbe-c27c104e2eed",
   "metadata": {},
   "source": [
    "## What we need to solve\n",
    "* Store the chat conversation.\n",
    "* When the user enters a new input, put that input in context.\n",
    "* Re-phrase the user input to have a contextualized input.\n",
    "* Send the contextualized input to the retriever.\n",
    "* Use the retriever to build a conversational rag chain.\n",
    "* Add extra features like persising memory (save memory in a file) and session memories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d049b49-4776-4ec4-aec5-02483ec25910",
   "metadata": {},
   "source": [
    "## The process we will follow\n",
    "1. Create a basic RAG without memory.\n",
    "2. Create a ChatPrompTemplate able to contextualize inputs.\n",
    "3. Create a retriever aware of memory.\n",
    "4. Create a basic conversational RAG.\n",
    "5. Create an advanced conversational RAG with persistence and session memories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46161e-45e9-46d7-8214-bcbea10aff2e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e0018-cba4-4959-881a-0a65093d202d",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell\n",
    "\n",
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook.\n",
    "\n",
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 001-conversational-rag.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74a7245-c5fe-4c4e-98e7-31b4708e8698",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863dd299-0780-49ad-a1b7-b76e249350da",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **001-conversational-rag**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b436a426-8b00-43f1-b523-60534a289f44",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4955a96-5e55-4d55-b92e-bf50538be227",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "033a1517-e787-4002-817c-7737e18a2be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd39d0-e72e-4bc2-8a68-2fa4008ea365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a923-b19e-498e-9be5-e47ec4a77d80",
   "metadata": {},
   "source": [
    "## Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c4cd4-2855-4708-9c94-6dfc4b25bff9",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1cf94ae-6c39-4475-9c5b-4b74d8d78753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e9e17-dfb0-4fd3-85b9-1fba83771941",
   "metadata": {},
   "source": [
    "## Connect with an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f22855-1656-49e8-b1d9-a352affbd814",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "148df8e0-361d-4ddd-8709-af48fa1648d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998155-91de-4cbc-bc88-8d77beefb51b",
   "metadata": {},
   "source": [
    "* NOTE: Since right now is the best LLM in the market, we will use OpenAI by default. You will see how to connect with other Open Source LLMs like Llama3 or Mistral in a next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae8595e-5c07-4b02-8a79-db55fd357527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a7e19-cf0b-4ffe-96d8-181be96ac986",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc942a2e-b9d3-4f5f-a612-f212cacc8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-community langchain-chroma bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfbec79-d2de-4d55-a2fa-e2125f914e56",
   "metadata": {},
   "source": [
    "## The process we will follow\n",
    "1. Create a basic RAG without memory.\n",
    "2. Create a ChatPrompTemplate able to contextualize inputs.\n",
    "3. Create a retriever aware of memory.\n",
    "4. Create a basic conversational RAG.\n",
    "5. Create an advanced conversational RAG with persistence and session memories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f596c-4904-49b2-ad65-7c10988d5bf5",
   "metadata": {},
   "source": [
    "## Step 1: Create a basic RAG without memory\n",
    "* We will use the RAG process we already know.\n",
    "* We will use create_stuff_documents_chain to build a qa chain: a chain able to asks questions to an LLM.\n",
    "* We will use create_retrieval_chain and the qa chain to build the RAG chain: a chain able to asks questions to the retriever and then format the response with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f29f71-3210-47b4-91d2-82ae30d11c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "#from langchain import hub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "text = TextLoader(\"data/be-good.txt\").load()\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vector_db = FAISS.from_documents(text, OpenAIEmbeddings())\n",
    "retriever = vector_db.as_retriever()\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b97dff6a-e703-4ed9-9701-3e08cbaabb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8616863f-6b53-40f3-bcd1-bc0dd73c5207",
   "metadata": {},
   "source": [
    "* Let's try the app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7772399b-c563-4a46-9051-2c66344c632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rag_chain.invoke({\"input\": \"What is this article about?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b471ef7-7723-4b24-9769-27381ada7dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article discusses the motto \"Make something people want\" coined by Y Combinator founders and how it relates to building successful businesses. It explores the idea of focusing on creating value for users before worrying about monetization, suggesting that this approach could resemble a charity model. Examples like Craigslist are used to illustrate this concept of running a successful business with a focus on user needs over profit.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804378d-7639-451e-95a8-b40f1419e907",
   "metadata": {},
   "source": [
    "* As we can see in the following question, our app has no memory of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34cc6fb8-0778-4df4-880f-d4957ff90c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rag_chain.invoke({\"input\": \"What was my previous question about?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6660b23a-4203-44c8-a516-84bf9b233f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your previous question was about the concept of benevolence in businesses and organizations, specifically how being benevolent can lead to success and growth. The idea was discussed in relation to examples such as Google, Microsoft, and Craigslist, highlighting the potential power of benevolence as a guiding principle.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b495ecd-9dba-45fe-b192-f2210ee4f382",
   "metadata": {},
   "source": [
    "## Step 2: Create a ChatPromptTemplate able to contextualize inputs\n",
    "* Goal: put the input in context and re-phrase it so we have a contextualized input.\n",
    "* We will define a new system prompt that instructs the LLM in how to contextualize the input.\n",
    "* Our new ChatPromptTemplate will include:\n",
    "    * The new system prompt.\n",
    "    * MessagesPlaceholder, a placeholder used to pass the list of messages included in the chat_history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeb66712-913d-4943-831b-ea80aeda1df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b770b5-da83-4687-a5b5-e01f34b0ee39",
   "metadata": {},
   "source": [
    "## Step 3: Create a Retriever aware of the memory\n",
    "* We will build our new retriever with create_history_aware_retriever that uses the contextualized input to get a contextualized response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74591ec2-2461-4a67-9187-82b3d052df67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0aeb46-3002-407a-950f-bd07d6eb0fdc",
   "metadata": {},
   "source": [
    "## Step 4: Create a basic Conversational RAG\n",
    "* We will use the retriever aware of memory, that uses the prompt with contextualized input.\n",
    "* We will use create_stuff_documents_chain to build a qa chain: a chain able to asks questions to an LLM.\n",
    "* We will use create_retrieval_chain and the qa chain to build the RAG chain: a chain able to asks questions to the retriever and then format the response with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b154434-7bdd-48e8-9438-5839c2a186d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "history_rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e119c-ece2-4686-af5b-1bdc1f1ea19f",
   "metadata": {},
   "source": [
    "#### Trying our basic conversational RAG\n",
    "Below we ask a question and a follow-up question that requires contextualization to return a sensible response. Because our chain includes a \"chat_history\" input, the caller needs to manage the chat history. We can achieve this by appending input and output messages to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4a1e69d-105f-492c-9b2f-c956522ababd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your previous question was about the topic or subject of the article under discussion.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "chat_history = []\n",
    "\n",
    "for i in range(0, 15):\n",
    "    query = str(input(\"Human : \"))\n",
    "    if query == \"exit\":\n",
    "        break\n",
    "    else :\n",
    "        response = history_rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "        print(response[\"answer\"])\n",
    "        chat_history.extend(\n",
    "            [\n",
    "                HumanMessage(content=query),\n",
    "                AIMessage(content=response[\"answer\"]),\n",
    "            ]\n",
    "        )\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0037a-d5cb-4813-8671-bc914401575e",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file Men6.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python Men6.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02-advanced-chatbot-8p_8ozoK-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
